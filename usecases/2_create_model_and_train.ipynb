{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079489b9-1eac-4dfc-891a-8aa7fa8b0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import  time\n",
    "import random\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread, imshow\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64719a2d-7339-46d6-996a-3bc25b7292ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = 128\n",
    "im_height = 128\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "num_class = 2\n",
    "\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_unet(input_img, n_filters = 64, dropout = 0.2, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(num_class, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "\n",
    "def TV_bin_loss(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    bin_loss = binary_crossentropy(y_true_f, y_pred_f)\n",
    "    images = y_pred[: : ,: ,1]\n",
    "    value = tf.reduce_mean(tf.abs(images[:,1:,:] - images[:,:-1,:])) + tf.reduce_mean(tf.abs(images[:,:,1:] - images[:,:,:-1]))\n",
    "    return 2.4e-7*value + bin_loss\n",
    "\n",
    "\n",
    "def dice_coef(y_pred, y_true):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 0.0001) / (K.sum(y_true_f) + K.sum(y_pred_f) + 0.0001)\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    layer_names=[layer.name for layer in model.layers]\n",
    "    for l in layer_names:\n",
    "        if l==layer_names[-1]:\n",
    "            value = TV_bin_loss(y_true, y_pred)\n",
    "        else:\n",
    "            value = binary_crossentropy(K.flatten(y_true),K.flatten(y_pred))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d72c01-7c5d-43eb-a72b-19a537a395cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = h5py.File(\"h5_datasets/combined_CT_datasets.h5\", \"r\")\n",
    "\n",
    "X_train = np.array(combined_data[\"X_train\"])\n",
    "X_valid = np.array(combined_data[\"X_valid\"])\n",
    "y_train = np.array(combined_data[\"y_train\"])\n",
    "y_valid = np.array(combined_data[\"y_valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66b655e-30d8-4cf8-9fd6-24f424a3dc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9462 - dice_loss: 0.2032 - recall_1: 0.9656 - pre_1: 0.8753\n",
      "Epoch 00001: val_loss improved from inf to 7.07172, saving model to model-TV-UNet1.h5\n",
      "41/41 [==============================] - 54s 829ms/step - loss: 0.2885 - accuracy: 0.9462 - dice_loss: 0.2032 - recall_1: 0.9656 - pre_1: 0.8753 - val_loss: 7.0717 - val_accuracy: 0.5510 - val_dice_loss: 0.4588 - val_recall_1: 0.5442 - val_pre_1: 0.5382 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9617 - dice_loss: 0.1126 - recall_1: 0.9617 - pre_1: 0.9616\n",
      "Epoch 00002: val_loss improved from 7.07172 to 2.19302, saving model to model-TV-UNet1.h5\n",
      "41/41 [==============================] - 22s 539ms/step - loss: 0.1586 - accuracy: 0.9617 - dice_loss: 0.1126 - recall_1: 0.9617 - pre_1: 0.9616 - val_loss: 2.1930 - val_accuracy: 0.7205 - val_dice_loss: 0.3406 - val_recall_1: 0.7594 - val_pre_1: 0.6679 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9630 - dice_loss: 0.0883 - recall_1: 0.9641 - pre_1: 0.9618\n",
      "Epoch 00003: val_loss improved from 2.19302 to 0.27677, saving model to model-TV-UNet1.h5\n",
      "41/41 [==============================] - 23s 553ms/step - loss: 0.1287 - accuracy: 0.9630 - dice_loss: 0.0883 - recall_1: 0.9641 - pre_1: 0.9618 - val_loss: 0.2768 - val_accuracy: 0.9490 - val_dice_loss: 0.1274 - val_recall_1: 0.9534 - val_pre_1: 0.9455 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9701 - dice_loss: 0.0688 - recall_1: 0.9708 - pre_1: 0.9692\n",
      "Epoch 00004: val_loss improved from 0.27677 to 0.23784, saving model to model-TV-UNet1.h5\n",
      "41/41 [==============================] - 24s 575ms/step - loss: 0.1014 - accuracy: 0.9701 - dice_loss: 0.0688 - recall_1: 0.9708 - pre_1: 0.9692 - val_loss: 0.2378 - val_accuracy: 0.9532 - val_dice_loss: 0.0815 - val_recall_1: 0.9553 - val_pre_1: 0.9509 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9753 - dice_loss: 0.0534 - recall_1: 0.9749 - pre_1: 0.9756\n",
      "Epoch 00005: val_loss did not improve from 0.23784\n",
      "41/41 [==============================] - 39s 955ms/step - loss: 0.0818 - accuracy: 0.9753 - dice_loss: 0.0534 - recall_1: 0.9749 - pre_1: 0.9756 - val_loss: 0.2941 - val_accuracy: 0.8962 - val_dice_loss: 0.1573 - val_recall_1: 0.9177 - val_pre_1: 0.8494 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9787 - dice_loss: 0.0454 - recall_1: 0.9784 - pre_1: 0.9788\n",
      "Epoch 00006: val_loss improved from 0.23784 to 0.13462, saving model to model-TV-UNet1.h5\n",
      "41/41 [==============================] - 48s 1s/step - loss: 0.0705 - accuracy: 0.9787 - dice_loss: 0.0454 - recall_1: 0.9784 - pre_1: 0.9788 - val_loss: 0.1346 - val_accuracy: 0.9673 - val_dice_loss: 0.0682 - val_recall_1: 0.9682 - val_pre_1: 0.9662 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9815 - dice_loss: 0.0375 - recall_1: 0.9812 - pre_1: 0.9817\n",
      "Epoch 00007: val_loss improved from 0.13462 to 0.11066, saving model to model-TV-UNet1.h5\n",
      "41/41 [==============================] - 53s 1s/step - loss: 0.0605 - accuracy: 0.9815 - dice_loss: 0.0375 - recall_1: 0.9812 - pre_1: 0.9817 - val_loss: 0.1107 - val_accuracy: 0.9728 - val_dice_loss: 0.0490 - val_recall_1: 0.9735 - val_pre_1: 0.9720 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9815 - dice_loss: 0.0342 - recall_1: 0.9812 - pre_1: 0.9818\n",
      "Epoch 00008: val_loss did not improve from 0.11066\n",
      "41/41 [==============================] - 56s 1s/step - loss: 0.0585 - accuracy: 0.9815 - dice_loss: 0.0342 - recall_1: 0.9812 - pre_1: 0.9818 - val_loss: 0.1110 - val_accuracy: 0.9642 - val_dice_loss: 0.0632 - val_recall_1: 0.9649 - val_pre_1: 0.9632 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9829 - dice_loss: 0.0311 - recall_1: 0.9827 - pre_1: 0.9830\n",
      "Epoch 00009: val_loss did not improve from 0.11066\n",
      "41/41 [==============================] - 55s 1s/step - loss: 0.0528 - accuracy: 0.9829 - dice_loss: 0.0311 - recall_1: 0.9827 - pre_1: 0.9830 - val_loss: 0.2249 - val_accuracy: 0.9139 - val_dice_loss: 0.0991 - val_recall_1: 0.9190 - val_pre_1: 0.9105 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9840 - dice_loss: 0.0285 - recall_1: 0.9837 - pre_1: 0.9843\n",
      "Epoch 00010: val_loss improved from 0.11066 to 0.05789, saving model to model-TV-UNet1.h5\n",
      "41/41 [==============================] - 58s 1s/step - loss: 0.0476 - accuracy: 0.9840 - dice_loss: 0.0285 - recall_1: 0.9837 - pre_1: 0.9843 - val_loss: 0.0579 - val_accuracy: 0.9819 - val_dice_loss: 0.0309 - val_recall_1: 0.9813 - val_pre_1: 0.9824 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "input_img = Input((im_height, im_width, 1), name='img')\n",
    "model = get_unet(input_img, n_filters=64, dropout=0.2, batchnorm=True)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=[custom_loss], metrics=['accuracy', dice_loss, Recall(name='recall_1'), Precision(name='pre_1')])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=50, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model-TV-UNet1.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "\n",
    "results = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, callbacks=callbacks, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce9ac48-b9be-4d11-9ce1-0b1412d213dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 10:21:21.662114: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "## save model as ONNX\n",
    "\n",
    "if not os.path.exists('tf_model'):\n",
    "    os.makedirs('tf_model')\n",
    "\n",
    "if not os.path.exists('onnx_trained_model'):\n",
    "    os.makedirs('onnx_trained_model')\n",
    "\n",
    "tf.saved_model.save(model, 'tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd6ec2d-eae9-4946-9087-9376e5f75e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-04-07 10:21:39,865 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-04-07 10:21:45,306 - INFO - Signatures found in model: [serving_default].\n",
      "2022-04-07 10:21:45,306 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-04-07 10:21:45,307 - INFO - Output names: ['conv2d_18']\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-04-07 10:21:49,299 - WARNING - From /opt/conda/lib/python3.9/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-04-07 10:21:51,788 - INFO - Using tensorflow=2.7.0, onnx=1.11.0, tf2onnx=1.9.3/1190aa\n",
      "2022-04-07 10:21:51,788 - INFO - Using opset <onnx, 14>\n",
      "2022-04-07 10:21:59,408 - INFO - Computed 0 values for constant folding\n",
      "2022-04-07 10:22:05,274 - INFO - Optimizing ONNX model\n",
      "2022-04-07 10:22:11,834 - INFO - After optimization: BatchNormalization -18 (18->0), Cast -4 (4->0), Concat -4 (8->4), Const -131 (178->47), Identity -13 (13->0), Reshape +1 (0->1), Shape -4 (4->0), Slice -4 (4->0), Squeeze -4 (4->0), Transpose -89 (90->1), Unsqueeze -16 (16->0)\n",
      "2022-04-07 10:22:12,456 - INFO - \n",
      "2022-04-07 10:22:12,456 - INFO - Successfully converted TensorFlow model tf_model to ONNX\n",
      "2022-04-07 10:22:12,456 - INFO - Model inputs: ['img']\n",
      "2022-04-07 10:22:12,456 - INFO - Model outputs: ['conv2d_18']\n",
      "2022-04-07 10:22:12,456 - INFO - ONNX model is saved at onnx_trained_model/ct_seg_model.onnx\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python -m tf2onnx.convert --saved-model tf_model --output onnx_trained_model/ct_seg_model.onnx --opset 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc5bca0-c0a8-468c-94f3-84f66dda780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release GPU memory\n",
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "if len(gpu) > 0:\n",
    "    from numba import cuda\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa091d-702e-406a-87e7-41cbd483d3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e48809-0fba-4377-a190-0ce4c77c600f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
